---
title:  "HOUSING PRICE: USING ADVANCED REGRESSION TECHNIQUES"
author:
- \emph{JIANWEN WU}
- \emph{NITASHA TUSI}
- \emph{THOMAS KWOK}
params:
 n: 100 


fontsize: 10pt

    
date: "`r format(Sys.time(), '%d %B %Y')`"
tags: [nothing, nothingness]

output: 
    pdf_document:
        toc: true
    fig_width: 4
    fig_height: 10
---


```{r load libraries, warning=F, message=F, echo=F}
library(recipes)
library(olsrr)
library(tidyverse)
library(pander)
library(recipes)
library(tidyverse)
library(stringr)
library(forcats)
```

```{r import data, warning=F, message=F, echo=F}
train_df <- read_csv("https://raw.githubusercontent.com/jianwenwu/glm-project/master/Data/training.csv")
test_df <- read_csv("https://raw.githubusercontent.com/jianwenwu/glm-project/master/Data/validation.csv")
```


```{r data cleaning, echo = F}

data_process <- function(data, train = T, test = F, cor_df = F){
  
  #convert into factor
  df <- data %>%
      mutate(MSSubClass = as.factor(MSSubClass),
             OverallCond = as.factor(OverallCond),
             YrSold = as.factor(YrSold), 
             MoSold = as.factor(MoSold),
             OverallQual = as.factor(OverallQual))
  
  #fill NA's With None ----
  NA_cols_None <- c("PoolQC",
                    "MiscFeature",
                    "Alley",
                    "Fence",
                    "FireplaceQu",
                    "GarageType",
                    "GarageFinish",
                    "GarageQual",
                    "GarageCond",
                    "BsmtQual",
                    "BsmtCond",
                    "BsmtExposure",
                    "BsmtFinType1",
                    "BsmtFinType2",
                    "MasVnrType",
                    "MSSubClass")
  
  df[,NA_cols_None][is.na(df[,NA_cols_None])] <- "None"
  
  #fill NA's with median of LotFrontage of Neighborhood ----
  df <- df %>%
    group_by(Neighborhood) %>%
    mutate(LotFrontage = replace_na(LotFrontage, replace = median(LotFrontage, na.rm = T))) %>%
    ungroup()
  
  
  #fill NA's with 0 ----
  NA_cols_0 <- (c(
    "GarageYrBlt",
    "GarageArea",
    "GarageCars",
    "BsmtFinSF1",
    "BsmtFinSF2",
    "BsmtUnfSF",
    "TotalBsmtSF",
    "BsmtFullBath",
    "BsmtHalfBath",
    "MasVnrArea"
  ))
  
  df[,NA_cols_0][is.na(df[,NA_cols_0])] <- 0
  
  #fill NA's with most frequently appear ----
  df <- df %>%
    mutate(Electrical = str_replace_na(string = Electrical, replacement = "SBrkr")) 
    
  if(train){
    
    return(
      
      df %>%
           #filter outlier ----  
           filter(GrLivArea < 4000) %>% 
        
        mutate(
          YearBuilt = ifelse(YearBuilt >= 1950, "New", "Old"),
      #Transform SalePrice into LogSalePrice ----
          Log_Sale_Price = log(SalePrice),
      #Create New Variable for TotalBathroom ----
          Total_Bathroom = BsmtFullBath + BsmtHalfBath + FullBath + HalfBath) %>%
        
        select(
          GrLivArea, GarageArea, TotalBsmtSF, TotRmsAbvGrd, BedroomAbvGr,
          YearBuilt, ExterQual, Neighborhood, BldgType, OverallQual, 
          HeatingQC, Total_Bathroom, Log_Sale_Price))
  }
  
  if(test){
    
    return(
      
      df %>%
        mutate(
          
          YearBuilt = ifelse(YearBuilt >= 1950, "New", "Old"),
      #Transform SalePrice into LogSalePrice ----
          Log_Sale_Price = log(SalePrice),
      #Create New Variable for TotalBathroom ----
          Total_Bathroom = BsmtFullBath + BsmtHalfBath + FullBath + HalfBath) %>%
        
        select(
          GrLivArea, GarageArea, TotalBsmtSF, TotRmsAbvGrd, BedroomAbvGr,
          YearBuilt, ExterQual, Neighborhood, BldgType, OverallQual, 
          HeatingQC, Total_Bathroom, Log_Sale_Price))
  }
  
  if(cor_df){
    
    return(
      df %>%
        mutate(Log_Sale_Price = log(SalePrice))
    )
  }
    
}

# run data_process function on train data
train_processed <- data_process(train_df,train = T)
plot_data <- data_process(train_df, train = F, test = F, cor_df = T)


#create dummy variables for all the nominal variables except variable "overallQual"
rec_obj <- recipe(Log_Sale_Price~., data = train_processed) %>%
  step_dummy(all_nominal(), -OverallQual) %>%
  prep()

#summary(rej_obj)

train_processed_model <- bake(rec_obj, new_data = train_processed)

```


#### INTRODUCTION:

Housing has always been seen as one of the major stepping stones in adulthood; people graduate college, get a job, start a family, and save enough money to buy a house. The question though has always been how much does it cost to buy a house and what factors into the price of a house. In this project, we looked into dataset from 1460 houses bought in Ames Iowa to create a regression model that best predicts the price of a house in Ames, Iowa. The variable of interest that we study is Sales Price and in our dataset we have eighty predictor variables that are utilized to predict the cost of a house. Of the eighty predictors, we have 23 nominal, 23 ordinal, 14 dis-crete, and 20 continuous and these range from everything from Total Plot Area to Fire Place and Pool. Our interest in this project is to select the most important variables to study and create a model that best predicts the Sales Price of a house. In evaluating our model, we will look into the bias of our model to the actual sales price, the maximal deviation, mean iabsolute deviation, and mean square error to conclude how accurately we created a model to predict the Sales Price. We will use 1060 observations in our training data set and compare our model to the 400 observations in the test data set to see how our modeled Sales Price compares to the actual Sales Price of the 400 observations.

From our dataset, we will look into normalizing the Sales Price by using the log function, and we will convert some of our predictors into dummy variables. From the list of variables, we will focus on several that we believe are highly correlated with SalesPrice, and then variable selection techniques also to see the maximum number of variables for predicting a highly accurate model without overfitting the data. The purpose is to create a model that can best predict the Sales Price from our validation data set, but also that could be used on other data sets and still get a good prediction. 



#### DATA PREPROCESSING 

Before we start to apply our regression model to the data set, it is essential for us to have a clean and tide training data, the better training data we can get, the better performance of our regression model could behave. In data pre-processing we are mostly detecting and dealing with missing values, handling outliers and replacing the values with the median or most frequent value Treatment given to each variable is described below.

*	Missing values in variables like PoolQC, MiscFeature, Alley, Fence, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, MasVnrType, MSSubClass are replaced by None based on data description.

*	Missing values in LotFrontage is replace by median of the data series.

*	Missing values in GarageYrBlt, GarageArea, GarageCars, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, To-talBsmtSF, BsmtFullBath, BsmtHalfBath, MasVnrArea are replaced with zero.

*	Electrical which denotes type of electrical sytem is replace by “SBrkr” because it was most frequent data val-ue in that series.

* Rest all categorical variables are converted into factors.


#### DATA EXPLORATION

After data cleaning and transformation we began with some exploratory analysis. A histogram plot shows the dis-tribution of the target variable ‘SalePrice’ as being was right-skewed. So we decided to take log of sale price in a way to obtain normal distribution for sale price.

###### Transformation of Sale Price
```{r sale price transformation, echo=F}

p1_saleprice_Hist <- plot_data %>%
  ggplot(aes()) +
  geom_histogram(aes(SalePrice), fill = "blue", 
                 color = "black", bins = 30)  +
  theme_bw() +
  labs(y = NULL, x = "Sale Price")

#QQ plot of SalePrice

p1_saleprice_QQ  <- plot_data %>%
  ggplot(aes(sample = SalePrice)) +
  stat_qq() +
  stat_qq_line() + 
  theme_bw() +
  labs(y = "Sale Price")




#Histogram of log SalePrice

p2_log_saleprice_Hist <- plot_data %>%
  ggplot(aes()) +
  geom_histogram(aes(Log_Sale_Price), fill = "blue", 
                 color = "black", bins = 30)  +
  theme_bw() +
  labs(y = NULL, x = "Log Sale Price")

#QQ plot of log SalePrice

p2_log_saleprice_QQ  <- plot_data %>%
  ggplot(aes(sample = Log_Sale_Price)) +
  stat_qq() +
  stat_qq_line() + 
  theme_bw() +
  labs(y = "Log Sale Price")




cowplot::plot_grid(p1_saleprice_Hist, p1_saleprice_QQ,
                   p2_log_saleprice_Hist, p2_log_saleprice_QQ,
                   ncol = 2)
```

We also noticed that most of the variables are positively skewed so as the sale price like 1stfloor surface area, sec-ond floor surface area, BedroomabvGr (bedrooms above basement level), LotFrontage, OpenporchSF etc.

```{r, echo = F}

p1_1stfl <- plot_data %>%
  ggplot(aes(X1stFlrSF)) +
  geom_histogram(fill = "blue", color = "black",bins = 30) +
  theme_bw() +
  labs(title = "First Floor square feet")


p2_2stfl <- plot_data %>%
  ggplot(aes(X2ndFlrSF)) +
  geom_histogram(fill = "blue", color = "black", bins = 30) +
  theme_bw() +
  labs(title = "Second Floor square feet")

cowplot::plot_grid(p1_1stfl, p2_2stfl,
                   ncol = 2)
```

While data exploration we noticed that some features had outliers like sale price was very huge for medium garage area. We have removed those outliers from the data. 


```{r outlier, echo = F}

p1_outlier <- plot_data %>%
  ggplot(aes(GarageArea, SalePrice)) +
  geom_point() +
  theme_bw() +
  labs(title = "before removed ouliters")


p2_outlier <- plot_data %>%
  filter(GrLivArea < 4000) %>%
  ggplot(aes(GarageArea, SalePrice)) +
  geom_point() +
  theme_bw() +
  labs(title = "after removed ouliters")

cowplot::plot_grid(p1_outlier, p2_outlier)
```

#### VARIABLE SELECTION

When selecting our variables, we decided first to see which predictors would someone who was buying a house choose. The ones that stood out right away were Living Area, Bathroom, number of bedrooms, and total rooms. When most people speak about houses, one of the first things they mention is the number of bedrooms and bath-rooms so we assumed those would be important factors in buying a house. For Living Area and total Basement Square Footage, we knew we did not need to use the 1st and 2nd floor breakdowns because they were added for total living area and total basement square footage. Below graph shows comparison between log of sale price and above the ground living area square feet. We can see some strong pattern between them.

```{r GrLiv Area vs Log sale price, echo=F}

train_processed %>%
  ggplot(aes(GrLivArea, Log_Sale_Price)) +
  geom_point() +
  geom_smooth(se = F, method = "lm") +
  labs(x = "Above the ground living area square feet",
       y = "Log of Sale Price") +
  theme_bw()

```

Next we included garage area and garage cars because of location, Iowa is a state that most people drive in so there will be a need for cars. Also, Iowa snows regularly, so an enclosed space, such as a garage, would be very benefi-cial for whomever is buying the house. This factor also made us include heating quality because Iowa is known to get very cold in the winters. 

```{r, echo = F}

p1 <- train_processed %>%
  ggplot(aes(HeatingQC)) +
  geom_bar(fill = 'blue') +
  labs(x = NULL, y = NULL,
       title = "Sum of Log Sale Price by HeatingQC") +
  theme_bw()


p2 <- train_processed %>%
  ggplot(aes(GarageArea, Log_Sale_Price)) +
  geom_point() +
  geom_smooth(se = F, method = "lm") +
  labs(x = "Garage Area",
       y = "Log of Sale Price") +
  theme_bw()

cowplot::plot_grid(p1, p2)

```


###### CORRELATION PLOT

```{r correlation plot, echo = F ,eval = F, fig.height = 9}
get_cor <- function(data, target, use = "pairwise.complete.obs",
                    fct_reorder = FALSE, fct_rev = FALSE) {
    
    feature_expr <- enquo(target)
    feature_name <- quo_name(feature_expr)
    
    data_cor <- data %>%
        mutate_if(is.character, as.factor) %>%
        mutate_if(is.factor, as.numeric) %>%
        cor(use = use) %>%
        as_tibble() %>%
        mutate(feature = names(.)) %>%
        select(feature, !! feature_expr) %>%
        filter(!(feature == feature_name)) %>%
        mutate_if(is.character, as_factor)
    
    if (fct_reorder) {
        data_cor <- data_cor %>% 
            mutate(feature = fct_reorder(feature, !! feature_expr)) %>%
            arrange(feature)
    }
    
    if (fct_rev) {
        data_cor <- data_cor %>% 
            mutate(feature = fct_rev(feature)) %>%
            arrange(feature)
    }
    
    return(data_cor)
    
}

plot_cor <- function(data, target, fct_reorder = FALSE, fct_rev = FALSE, 
                     include_lbl = TRUE, lbl_precision = 2, lbl_position = "outward",
                     size = 2, line_size = 1, vert_size = 1, 
                     color_pos = palette_light()[[1]], color_neg = palette_light()[[2]]) {
    
    feature_expr <- enquo(target)
    feature_name <- quo_name(feature_expr)
    
    data_cor <- data %>%
        get_cor(!! feature_expr, fct_reorder = fct_reorder, fct_rev = fct_rev) %>%
        mutate(feature_name_text = round(!! feature_expr, lbl_precision)) %>%
        mutate(Correlation = case_when(
            (!! feature_expr) >= 0 ~ "Positive",
            TRUE                   ~ "Negative") %>% as.factor())
    
    g <- data_cor %>%
        ggplot(aes_string(x = feature_name, y = "feature", group = "feature")) +
        geom_point(aes(color = Correlation), size = size) +
        geom_segment(aes(xend = 0, yend = feature, color = Correlation), size = line_size) +
        geom_vline(xintercept = 0, color = palette_light()[[1]], size = vert_size) +
        expand_limits(x = c(-1, 1)) +
        theme_tq() +
        scale_color_manual(values = c(color_neg, color_pos)) 
    
    if (include_lbl) g <- g + geom_label(aes(label = feature_name_text), hjust = lbl_position)
    
    return(g)
    
}


plot_data %>%
  select_if(is.numeric) %>%
  plot_cor(Log_Sale_Price,fct_reorder = T)
  
```

#### NEW VARIABLE CREATION AND TRANSFORMATION

The dataset contains rich information about all the factors which a home buyer will consider while buying a house. We created new intuitive variables to draw more meaningful insights. 

* Total number of bathrooms : we decided to create our own variable for bathrooms, by adding up basement half and full bathroom, and above ground half and full bathroom. The reason why we decided to create a new variable called total bathroom, was because we felt they each were important but we also just didn’t need four separate variables for bathroom. 

* Two categories for year of built: Year of built had many different years. We decided to split it into two variables, old and new. If a house was built before 1950, it was considered old and if it was built after 1950, we considered it new. The reason we chose 1950 was because when we compared year built with sales price, we saw a huge increase in sale price after 1950. 

#### MODEL

Our final list of variables is as follows

*	GrLivArea(1st + 2nd SF): Above the ground living surface area. 

*	GarageArea: Size of garage in square feet.

*	TotalBsmtSF(BSMT 1st + 2nd): Total square feet of basement area.

* Total Bathroom: Add 4 bathroom variables up

*	GarageCars: Size of garage in car capacity. 

*	TotRmsAbvGrd: Total rooms above the ground does not include bathroom

*	BedroomAbvGr: Number of bedrooms

*	YearBuilt(categorical): Divided this variable in two categories old and new based.

*	ExterQual(categorical): Exterior material quality

*	Neighborhood(categorical): Physical location  

*	BldgType(categorical): Type of dwelling

*	OverallQual(categorical): Overall material and finish quality

*	HeatingQC(categorical) : Heating quality and condition


We decide to spilt categorical variables into dummy variables except the predictor "OverallQual". In total we had fourteen variables chosen, but after making dummy variables, this number increased to 43.


#### MODEL DEVELOPMENT.
We developed a multiple linear regression model with all 43 variables. The R Square is 0.8869 and adjust R Square is 0.8812. 
```{r lm, echo = F,results='hide'}
lm_fit <- lm(Log_Sale_Price ~., data = train_processed_model) 

summary(lm_fit)

```

The model was too complexity and was hard to explain. we perfomerd a stepwise AIC forward regression on these 43 variables and reduced number of variables to 27.  The new model would be less complexity while retained predictive power.  The stepwise AIC forward regression will iteratively adding variable to the model until the AIC would not be decrease.  In our case, the stepwise AIC forward regression stop at variable #27 'Neighborhood_BrkSide' with Adjust R Square 0.88176 (base on the result below) . 

```{r forward aic,echo = F, warning = F, message = F}
lm_fw_aic <- ols_step_forward_aic(lm_fit)
lm_fw_aic
```

From the result above, we can see that the variable 'OverallQual' is most important variable with R square 0.67046. A single variable explained 67% of variance of the data!  Follow by "GrLivArea", "YearBuilt_Old", and so on.


```{r plot - no.variables vs adj r2, echo  = F}
fd_df <- tibble(
  rank = c(seq(1,27,1)),
  predictor = lm_fw_aic$predictors, 
  adj_r2 = lm_fw_aic$arsq) 


#plot No. of Variables vs Adjust R2

fd_df %>%
  ggplot(aes(rank, adj_r2)) +
  geom_point() +
  geom_line() +
  labs(x = "Number of Variables", y = "Adjust R2") +
  scale_x_continuous(breaks = c(seq(1,27,1))) +
  theme_bw()

```

In the plot above, we plotted all 27 variables that was chosen by stepwise AIC forward regression corresponding to adjusted R square. The plot shows as the number of variables increased, the adjust R-Squared increased as well.  we know that adjusted R-Squared is modified version of R-squared that has been adjusted for the number of predictors in the model, and it penalized number of predictors was added.  In conclusion, we decided to keep all 27 variables.


#### THE RESPONSE FUNCTION
```{r responsed function, echo=F}
lm_fw_aic$model %>%
  pander::pander()
```

We can see from the responded function that neighborhood like StoneBr, Timber, Brkside, Somrest shows a positive relationship with sale price whereas neighbor like meadow and Brdale shows negative relationship with log sale price


### MODEL DIAGNOSTICS

We need to check the assumptions below.

Multiple Linear Regression Assumptions: 

* The errors has normal distribution
* The errors has mean 0
* Homoscedasticity of errors or equal variance
* The errors are independent.


###### Residual QQ plot and Residual Histogram
```{r, echo = F}
#QQplot
ols_plot_resid_qq(lm_fw_aic$model)

#Histogram
ols_plot_resid_hist(lm_fw_aic$model)

```

The QQ plot -  The residual points roughly lie within the lines. The Q-Q plot of the residuals suggests that the error terms are indeed normally distributed.

The histogram - The errors terms are indeed normally distributed.


###### Residual vs Fitted Values Plot.
```{r, echo =F }
ols_plot_resid_fit(lm_fw_aic$model) 
```

* The residuals spread randomly around the 0 line indicating that the relationship is linear.

* The residuals roughly horizontal band around the 0 line indicating homogeneity of error variance.(constant variance)

* No residuals are away from random pattern of residuals indicating no outliers.

###### Correlation between actual log sale price and predicted log sale price

```{r, eval  =F}
tibble(
  actual = train_processed_model$Log_Sales_Price,
  predicted = lm_fw_aic$model$fitted.values
) %>%
  ggplot(aes(actual, predicted)) +
  geom_point() +
  geom_smooth(se = F, method = "lm") +
  labs(x = "actual log sale price", y = "predicted log sale price") +
  theme_bw()

cor(train_processed_model$Log_Sales_Price, 
    lm_fw_aic$model$fitted.values)
```

Based on the graph above, we can see that our model performed very good.  The correlation between actual log sale price and predicted log sale price is 0.94.

Overall, our model satisfied the linear regression assumptions.

### PREDICTION ON TEST SET

We are going to use the 4 metrics below to measure performance of our model on the test dataset(using sale price). We transformed log sale price back to sale price. 

* Bias
* Maximum Deviation 
* Mean Absolute Deviation 
* Mean Square Error

```{r, echo = F}
bias <- function(Y_hat, Y){
  mean(Y_hat - Y)
}

Max_Dev <- function(Y_hat, Y){
  max(abs(Y_hat - Y))
}

Mean_Abs_Dev <- function(Y_hat, Y){
  mean(abs(Y_hat - Y))
}

Mean_sq_err <- function(Y_hat , Y){
  mean((Y_hat - Y)^2)
} 

test_processed <- data_process(test_df, train = F, test = T)
test_processed_model <- bake(rec_obj, new_data = test_processed)

# predction on log sale price
predicted_value_log <- lm_fw_aic$model %>%
  predict(test_processed_model)

# prediction on sale price
predicted_value <- exp(predicted_value_log)


```


```{r, echo = F}

y_hat_test = predicted_value
y_test = test_df$SalePrice

#test
tibble(
  test_bias = bias(y_hat_test, y_test),
  test_Max_Dev = Max_Dev(y_hat_test, y_test),
  test_Mean_Abs_Dev = Mean_Abs_Dev(y_hat_test, y_test),
  test_Mean_Sq_Err = Mean_sq_err(y_hat_test, y_test)
) %>%
  gather(key = "measure", value = "value") %>%
  pander::pander()

```

The table above is our model performance on test set.



```{r, eval=F}
#predicted_value %>%
# write.csv(predicted_value, "Predictied_Value.csv", na = "", row.names = F)
```

Conclusion:

In conclusion, by running a linear model with our 27 variables, we found that our adjusted R-square was .88176. We determined that the error terms were normally distributed and that our relationship was linear, which supported the choice of our model. We found a strong correlation between actual log sale price and predicted log sale price. The model performance on our test set is shown above and in the end we found a good combination of numeric and categoric variables led to the best prediction for Sales Price. 
